{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import xgboost\n",
    "import utils\n",
    "import scoring\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "columns =  [\"id\", \"label\", \"weight\", \"sWeight\", \"kinWeight\"]\n",
    "FOI_Names = ['closest_x_0','closest_x_1','closest_x_2','closest_x_3','closest_y_0','closest_y_1','closest_y_2','closest_y_3',\n",
    "            'closest_T_0','closest_T_1','closest_T_2','closest_T_3','closest_z_0','closest_z_1','closest_z_2','closest_z_3',\n",
    "            'closest_dx_0','closest_dx_1','closest_dx_2','closest_dx_3','closest_dy_0','closest_dy_1','closest_dy_2','closest_dy_3']\n",
    "# DATA_PATH = \".\"\n",
    "DATA_PATH = \"/Users/mykola/MLHEP/MLHEP-2020-muon-id\"\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"train.csv.gz\"),index_col=\"id\", usecols=columns+utils.SIMPLE_FEATURE_COLUMNS)\n",
    "foi_train = pd.read_csv(os.path.join(DATA_PATH, \"transformed_foi.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0  closest_x_0   closest_x_1   closest_x_2   closest_x_3  \\\n",
      "0                 0  8110.418500  12362.430000  17592.045000  33490.310000   \n",
      "1                 1  2565.165300   4954.634000  17085.285000  11960.113000   \n",
      "2                 2    27.115717     41.721634   2870.406200    408.955800   \n",
      "3                 3    67.807945      7.291055    675.428830    790.541930   \n",
      "4                 4     9.596987     19.489336  13997.559000  10585.928000   \n",
      "...             ...          ...           ...           ...           ...   \n",
      "1263649     1263649    12.909776    143.347430      8.835234    872.462460   \n",
      "1263650     1263650    26.929157     60.519360    123.140366    495.869500   \n",
      "1263651     1263651    39.359080    115.675385   1000.000000  36999.920000   \n",
      "1263652     1263652  1445.503300   4386.151400   3480.251000     40.008095   \n",
      "1263653     1263653   132.106860      2.306750    319.738220     24.365648   \n",
      "\n",
      "          closest_y_0  closest_y_1   closest_y_2   closest_y_3  closest_T_0  \\\n",
      "0          450.712070   650.299800    888.188400  1.184664e+03          7.0   \n",
      "1           77.468790  4349.476600   4904.615000  5.539887e+03          4.0   \n",
      "2          208.075000   174.103400    176.514830  2.193885e+02          7.0   \n",
      "3          322.047400   362.576540    408.305600  4.391793e+02          4.0   \n",
      "4           16.624134    24.616957  14863.664000  1.702675e+04          4.0   \n",
      "...               ...          ...           ...           ...          ...   \n",
      "1263649     58.465923   870.523700  13138.835000  1.462166e+04          6.0   \n",
      "1263650     30.013464    38.174960     48.274677  5.332636e+01          6.0   \n",
      "1263651  15934.162000  9611.155000   1000.000000  1.614222e+06         12.0   \n",
      "1263652     82.177030   109.082880    118.207220  6.898152e+01          4.0   \n",
      "1263653   1051.866700  1247.309000   1375.004600  1.392031e+03          3.0   \n",
      "\n",
      "         ...  closest_z_2  closest_z_3  closest_dx_0  closest_dx_1  \\\n",
      "0        ...    17714.242    18922.621        12.750        13.750   \n",
      "1        ...    17713.979    18922.340        12.750        27.500   \n",
      "2        ...    17809.107    19017.820        25.500        27.500   \n",
      "3        ...    17606.070    18815.047         6.375         6.875   \n",
      "4        ...    17801.920    19010.550        12.750        13.750   \n",
      "...      ...          ...          ...           ...           ...   \n",
      "1263649  ...    17712.133    18919.970         6.375        13.750   \n",
      "1263650  ...    17608.957    18818.133         6.375         6.875   \n",
      "1263651  ...     1000.000    18812.809        12.750         6.875   \n",
      "1263652  ...    17714.510    18922.512        25.500        27.500   \n",
      "1263653  ...    17518.607    18726.953         6.375         6.875   \n",
      "\n",
      "         closest_dx_2  closest_dx_3  closest_dy_0  closest_dy_1  closest_dy_2  \\\n",
      "0                59.0          63.0     63.078957     68.078926     73.078896   \n",
      "1               118.0         126.0     63.078957    136.278490    146.278410   \n",
      "2               118.0         126.0    126.278550    136.278490    146.278410   \n",
      "3                29.5          31.5     31.479162     33.979145     36.479130   \n",
      "4                59.0          63.0     63.078957     68.078926     73.078896   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "1263649          59.0          63.0     31.479162     68.078926     73.078896   \n",
      "1263650          29.5          31.5     31.479162     33.979145     36.479130   \n",
      "1263651        1000.0          31.5     63.078957     33.979145   1000.000000   \n",
      "1263652         118.0         126.0    126.278550    136.278490    146.278410   \n",
      "1263653          29.5          31.5     31.479162     33.979145     36.479130   \n",
      "\n",
      "         closest_dy_3  \n",
      "0           78.078860  \n",
      "1          156.278350  \n",
      "2          156.278350  \n",
      "3           38.979115  \n",
      "4           78.078860  \n",
      "...               ...  \n",
      "1263649     78.078860  \n",
      "1263650     38.979115  \n",
      "1263651     38.979115  \n",
      "1263652    156.278350  \n",
      "1263653     38.979115  \n",
      "\n",
      "[1263654 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "col =  [ \"label\", \"weight\", \"sWeight\", \"kinWeight\"]\n",
    "\n",
    "print(foi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input = pd.concat([train, foi_train], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ncl[0]  ncl[1]  ncl[2]  ncl[3]  avg_cs[0]  avg_cs[1]  avg_cs[2]  \\\n",
      "0            79      18      22      10   2.392405   2.055556   1.590909   \n",
      "1            33      14       6      15   1.575758   2.285714   1.333333   \n",
      "2           115      53      23      25   2.843478   2.433962   1.826087   \n",
      "3            28       8       5      15   2.964286   1.500000   1.800000   \n",
      "4            50       8       4      13   2.400000   1.250000   1.500000   \n",
      "...         ...     ...     ...     ...        ...        ...        ...   \n",
      "1263649      40      23      12      12   2.325000   1.391304   1.166667   \n",
      "1263650      34       8       8       9   3.323529   1.500000   1.625000   \n",
      "1263651      25       2       9      10   1.480000   1.000000   1.222222   \n",
      "1263652      56      11       5       6   4.607143   1.727273   2.800000   \n",
      "1263653     141      42      23      33   2.411348   2.119048   1.478261   \n",
      "\n",
      "         avg_cs[3]  ndof  MatchedHit_TYPE[0]  ...  closest_z_2  closest_z_3  \\\n",
      "0         1.200000     8                   2  ...    17714.242    18922.621   \n",
      "1         1.400000     8                   2  ...    17713.979    18922.340   \n",
      "2         1.280000     8                   2  ...    17809.107    19017.820   \n",
      "3         1.000000     8                   2  ...    17606.070    18815.047   \n",
      "4         1.076923     8                   2  ...    17801.920    19010.550   \n",
      "...            ...   ...                 ...  ...          ...          ...   \n",
      "1263649   1.083333     8                   2  ...    17712.133    18919.970   \n",
      "1263650   1.333333     8                   2  ...    17608.957    18818.133   \n",
      "1263651   1.300000     6                   2  ...     1000.000    18812.809   \n",
      "1263652   1.166667     8                   2  ...    17714.510    18922.512   \n",
      "1263653   1.272727     8                   2  ...    17518.607    18726.953   \n",
      "\n",
      "         closest_dx_0  closest_dx_1  closest_dx_2  closest_dx_3  closest_dy_0  \\\n",
      "0              12.750        13.750          59.0          63.0     63.078957   \n",
      "1              12.750        27.500         118.0         126.0     63.078957   \n",
      "2              25.500        27.500         118.0         126.0    126.278550   \n",
      "3               6.375         6.875          29.5          31.5     31.479162   \n",
      "4              12.750        13.750          59.0          63.0     63.078957   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "1263649         6.375        13.750          59.0          63.0     31.479162   \n",
      "1263650         6.375         6.875          29.5          31.5     31.479162   \n",
      "1263651        12.750         6.875        1000.0          31.5     63.078957   \n",
      "1263652        25.500        27.500         118.0         126.0    126.278550   \n",
      "1263653         6.375         6.875          29.5          31.5     31.479162   \n",
      "\n",
      "         closest_dy_1  closest_dy_2  closest_dy_3  \n",
      "0           68.078926     73.078896     78.078860  \n",
      "1          136.278490    146.278410    156.278350  \n",
      "2          136.278490    146.278410    156.278350  \n",
      "3           33.979145     36.479130     38.979115  \n",
      "4           68.078926     73.078896     78.078860  \n",
      "...               ...           ...           ...  \n",
      "1263649     68.078926     73.078896     78.078860  \n",
      "1263650     33.979145     36.479130     38.979115  \n",
      "1263651     33.979145   1000.000000     38.979115  \n",
      "1263652    136.278490    146.278410    156.278350  \n",
      "1263653     33.979145     36.479130     38.979115  \n",
      "\n",
      "[1263654 rows x 89 columns]\n",
      "           ncl[0]    ncl[1]    ncl[2]    ncl[3]  avg_cs[0]  avg_cs[1]  \\\n",
      "0        0.502109 -0.098661  1.290342 -0.415117  -0.128005  -0.097966   \n",
      "1       -0.899662 -0.407158 -0.813614  0.357205  -1.332468   0.187420   \n",
      "2        1.599146  2.600691  1.421840  1.901849   0.537277   0.371241   \n",
      "3       -1.052028 -0.869904 -0.945112  0.357205   0.715454  -0.786829   \n",
      "4       -0.381616 -0.869904 -1.076609  0.048276  -0.116804  -1.096817   \n",
      "...           ...       ...       ...       ...        ...        ...   \n",
      "1263649 -0.686349  0.286961 -0.024631 -0.106188  -0.227420  -0.921607   \n",
      "1263650 -0.869189 -0.869904 -0.550620 -0.569582   1.245298  -0.786829   \n",
      "1263651 -1.143448 -1.332650 -0.419123 -0.415117  -1.473700  -1.406806   \n",
      "1263652 -0.198777 -0.638531 -0.945112 -1.032975   3.138484  -0.505022   \n",
      "1263653  2.391451  1.752323  1.421840  3.137565  -0.100067  -0.019239   \n",
      "\n",
      "         avg_cs[2]  avg_cs[3]      ndof  MatchedHit_TYPE[0]  ...  closest_z_2  \\\n",
      "0         0.364089  -0.359421  0.235168            0.319909  ...     0.204816   \n",
      "1        -0.308850   0.081468  0.235168            0.319909  ...     0.204729   \n",
      "2         0.978512  -0.183065  0.235168            0.319909  ...     0.236189   \n",
      "3         0.910357  -0.800310  0.235168            0.319909  ...     0.169042   \n",
      "4         0.126581  -0.630737  0.235168            0.319909  ...     0.233812   \n",
      "...            ...        ...       ...                 ...  ...          ...   \n",
      "1263649  -0.744282  -0.616606  0.235168            0.319909  ...     0.204118   \n",
      "1263650   0.453154  -0.065495  0.235168            0.319909  ...     0.169997   \n",
      "1263651  -0.599138  -0.138976 -3.596482            0.319909  ...    -5.322732   \n",
      "1263652   3.522944  -0.432902  0.235168            0.319909  ...     0.204905   \n",
      "1263653   0.069786  -0.199097  0.235168            0.319909  ...     0.140118   \n",
      "\n",
      "         closest_z_3  closest_dx_0  closest_dx_1  closest_dx_2  closest_dx_3  \\\n",
      "0           0.185368     -0.057853     -0.050222     -0.154273     -0.126167   \n",
      "1           0.185271     -0.057853      0.764098      0.180575      0.266335   \n",
      "2           0.218023      0.648885      0.764098      0.180575      0.266335   \n",
      "3           0.148467     -0.411222     -0.457383     -0.321697     -0.322417   \n",
      "4           0.215529     -0.057853     -0.050222     -0.154273     -0.126167   \n",
      "...              ...           ...           ...           ...           ...   \n",
      "1263649     0.184458     -0.411222     -0.050222     -0.154273     -0.126167   \n",
      "1263650     0.149526     -0.411222     -0.457383     -0.321697     -0.322417   \n",
      "1263651     0.147700     -0.057853     -0.457383      5.186273     -0.322417   \n",
      "1263652     0.185330      0.648885      0.764098      0.180575      0.266335   \n",
      "1263653     0.118249     -0.411222     -0.457383     -0.321697     -0.322417   \n",
      "\n",
      "         closest_dy_0  closest_dy_1  closest_dy_2  closest_dy_3  \n",
      "0            0.042430      0.048936     -0.142042     -0.115535  \n",
      "1            0.042430      1.523473      0.274843      0.370274  \n",
      "2            1.440683      1.523473      0.274843      0.370274  \n",
      "3           -0.656696     -0.688333     -0.350484     -0.358440  \n",
      "4            0.042430      0.048936     -0.142042     -0.115535  \n",
      "...               ...           ...           ...           ...  \n",
      "1263649     -0.656696      0.048936     -0.142042     -0.115535  \n",
      "1263650     -0.656696     -0.688333     -0.350484     -0.358440  \n",
      "1263651      0.042430     -0.688333      5.136937     -0.358440  \n",
      "1263652      1.440683      1.523473      0.274843      0.370274  \n",
      "1263653     -0.656696     -0.688333     -0.350484     -0.358440  \n",
      "\n",
      "[1263654 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "nn_input.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names] = sc.fit_transform(nn_input.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names])\n",
    "print (nn_input.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def constrained_MSE(y_true,y_pred):\n",
    "    return K.mean(K.square(tf.sigmoid(y_pred) - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorlayer as tl\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal', input_dim=89))\n",
    "model.add(layers.Dropout(rate=0.45))\n",
    "model.add(Dense(512, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal'))\n",
    "model.add(layers.Dropout(rate=0.35))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(Dense(256, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal'))\n",
    "model.add(layers.Dropout(rate=0.25))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(Dense(256, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal'))\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(Dense(128, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal'))\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(Dense(128, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal'))\n",
    "#model.add(layers.Dropout(rate=0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(Dense(64, activation= lambda x : tl.act.lrelu(x, 0.1), kernel_initializer='random_normal'))\n",
    "model.add(layers.BatchNormalization())\n",
    "#model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#model.compile(optimizer =Adam(lr=0.001),loss='binary_crossentropy', metrics =['accuracy','binary_crossentropy'],weighted_metrics=['accuracy'])\n",
    "model.compile(optimizer =Adam(lr=0.001),loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200471, 94)\n",
      "(63183, 94)\n"
     ]
    }
   ],
   "source": [
    "train_part, validation = train_test_split(nn_input, test_size=0.05, shuffle=True, random_state=2342234)\n",
    "print(train_part.shape)\n",
    "print(validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130311    0\n",
      "819219     0\n",
      "155811     1\n",
      "407242     1\n",
      "164689     0\n",
      "          ..\n",
      "797663     0\n",
      "302199     1\n",
      "496404     1\n",
      "1028939    1\n",
      "397432     1\n",
      "Name: label, Length: 63183, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(validation.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2387/2401 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.0000e+00 - binary_crossentropy: 10.3816\n",
      "Epoch 00001: saving model to NNmodels/weights.01-0.38.h5\n",
      "2401/2401 [==============================] - 6s 2ms/step - loss: 0.3867 - accuracy: 0.0000e+00 - binary_crossentropy: 10.4018 - val_loss: 0.3850 - val_accuracy: 0.0000e+00 - val_binary_crossentropy: 13.3058\n",
      "Epoch 2/200\n",
      "2389/2401 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.0000e+00 - binary_crossentropy: 15.0672\n",
      "Epoch 00002: saving model to NNmodels/weights.02-0.38.h5\n",
      "2401/2401 [==============================] - 5s 2ms/step - loss: 0.3867 - accuracy: 0.0000e+00 - binary_crossentropy: 15.0488 - val_loss: 0.3850 - val_accuracy: 0.0000e+00 - val_binary_crossentropy: 12.6085\n",
      "Epoch 3/200\n",
      "2387/2401 [============================>.] - ETA: 0s - loss: 0.3865 - accuracy: 0.0000e+00 - binary_crossentropy: 16.3084\n",
      "Epoch 00003: saving model to NNmodels/weights.03-0.38.h5\n",
      "2401/2401 [==============================] - 6s 2ms/step - loss: 0.3866 - accuracy: 0.0000e+00 - binary_crossentropy: 16.3083 - val_loss: 0.3849 - val_accuracy: 0.0000e+00 - val_binary_crossentropy: 16.7646\n",
      "Epoch 4/200\n",
      "2381/2401 [============================>.] - ETA: 0s - loss: 0.3865 - accuracy: 0.0000e+00 - binary_crossentropy: 14.9760\n",
      "Epoch 00004: saving model to NNmodels/weights.04-0.38.h5\n",
      "2401/2401 [==============================] - 6s 2ms/step - loss: 0.3865 - accuracy: 0.0000e+00 - binary_crossentropy: 14.9505 - val_loss: 0.3848 - val_accuracy: 0.0000e+00 - val_binary_crossentropy: 12.7031\n",
      "Epoch 5/200\n",
      "2393/2401 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.0000e+00 - binary_crossentropy: 10.6675\n",
      "Epoch 00005: saving model to NNmodels/weights.05-0.38.h5\n",
      "2401/2401 [==============================] - 6s 2ms/step - loss: 0.3863 - accuracy: 0.0000e+00 - binary_crossentropy: 10.6677 - val_loss: 0.3848 - val_accuracy: 0.0000e+00 - val_binary_crossentropy: 13.0569\n",
      "Epoch 6/200\n",
      "2140/2401 [=========================>....] - ETA: 0s - loss: 0.3864 - accuracy: 0.0000e+00 - binary_crossentropy: 13.1184"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-6f1d1e40baf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_all_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NNmodels/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'weights.{epoch:02d}-{val_loss:.2f}.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#history = model.fit(train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, train_part.label, verbose=1, callbacks=[save_all_xy], validation_data=(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, validation.label),  shuffle=True, batch_size=700, epochs=160)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIMPLE_FEATURE_COLUMNS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFOI_Names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msWeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_all_xy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIMPLE_FEATURE_COLUMNS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFOI_Names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#history = model.fit(train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values, train_part.label, verbose=1, callbacks=[save_all_xy],  sample_weight=train_part.weight.values, validation_data=(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values, validation.label),  shuffle=True, batch_size=700, epochs=500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_all_xy = keras.callbacks.ModelCheckpoint(\"NNmodels/\"+'weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#history = model.fit(train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, train_part.label, verbose=1, callbacks=[save_all_xy], validation_data=(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, validation.label),  shuffle=True, batch_size=700, epochs=160)\n",
    "history = model.fit(train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, train_part.sWeight, verbose=1, callbacks=[save_all_xy], validation_data=(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, validation.sWeight),  shuffle=True, batch_size=500, epochs=200)\n",
    "#history = model.fit(train_part.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values, train_part.label, verbose=1, callbacks=[save_all_xy],  sample_weight=train_part.weight.values, validation_data=(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS].values, validation.label),  shuffle=True, batch_size=700, epochs=500)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss', 'Validation loss'], loc='upper right')\n",
    "plt.savefig(\"loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975/1975 [==============================] - 3s 2ms/step - loss: 0.2281 - accuracy: 0.6676 - binary_crossentropy: 3.7246 - weighted_accuracy: 0.6676\n",
      "[0.228145033121109, 0.6676163077354431, 3.7246248722076416, 0.6676163077354431]\n"
     ]
    }
   ],
   "source": [
    "eval_model=model.evaluate(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values, validation.label)\n",
    "print(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weights.01-0.42.h5', 'weights.02-0.40.h5', 'weights.03-0.41.h5', 'weights.04-0.40.h5', 'weights.05-0.40.h5', 'weights.06-0.40.h5', 'weights.07-0.39.h5', 'weights.08-0.40.h5', 'weights.09-0.39.h5', 'weights.10-0.39.h5', 'weights.100-0.38.h5', 'weights.1000-0.37.h5', 'weights.101-0.38.h5', 'weights.102-0.38.h5', 'weights.103-0.38.h5', 'weights.104-0.38.h5', 'weights.105-0.38.h5', 'weights.106-0.38.h5', 'weights.107-0.38.h5', 'weights.108-0.38.h5', 'weights.109-0.38.h5', 'weights.11-0.39.h5', 'weights.110-0.38.h5', 'weights.111-0.38.h5', 'weights.112-0.38.h5', 'weights.113-0.38.h5', 'weights.114-0.38.h5', 'weights.115-0.38.h5', 'weights.116-0.38.h5', 'weights.117-0.38.h5', 'weights.118-0.38.h5', 'weights.119-0.38.h5', 'weights.12-0.39.h5', 'weights.120-0.38.h5', 'weights.121-0.38.h5', 'weights.122-0.38.h5', 'weights.123-0.38.h5', 'weights.124-0.38.h5', 'weights.125-0.38.h5', 'weights.126-0.38.h5', 'weights.127-0.38.h5', 'weights.128-0.38.h5', 'weights.129-0.38.h5', 'weights.13-0.39.h5', 'weights.130-0.38.h5', 'weights.131-0.38.h5', 'weights.132-0.38.h5', 'weights.133-0.38.h5', 'weights.134-0.38.h5', 'weights.135-0.38.h5', 'weights.136-0.38.h5', 'weights.137-0.38.h5', 'weights.138-0.38.h5', 'weights.139-0.38.h5', 'weights.14-0.39.h5', 'weights.140-0.38.h5', 'weights.141-0.38.h5', 'weights.142-0.38.h5', 'weights.143-0.38.h5', 'weights.144-0.38.h5', 'weights.145-0.38.h5', 'weights.146-0.38.h5', 'weights.147-0.38.h5', 'weights.148-0.38.h5', 'weights.149-0.38.h5', 'weights.15-0.39.h5', 'weights.150-0.38.h5', 'weights.151-0.38.h5', 'weights.152-0.38.h5', 'weights.153-0.38.h5', 'weights.154-0.38.h5', 'weights.155-0.38.h5', 'weights.156-0.38.h5', 'weights.157-0.38.h5', 'weights.158-0.38.h5', 'weights.159-0.38.h5', 'weights.16-0.39.h5', 'weights.160-0.38.h5', 'weights.161-0.38.h5', 'weights.162-0.38.h5', 'weights.163-0.38.h5', 'weights.164-0.38.h5', 'weights.165-0.38.h5', 'weights.166-0.38.h5', 'weights.167-0.38.h5', 'weights.168-0.38.h5', 'weights.169-0.38.h5', 'weights.17-0.39.h5', 'weights.170-0.38.h5', 'weights.171-0.38.h5', 'weights.172-0.38.h5', 'weights.173-0.38.h5', 'weights.174-0.38.h5', 'weights.175-0.38.h5', 'weights.176-0.38.h5', 'weights.177-0.38.h5', 'weights.178-0.38.h5', 'weights.179-0.38.h5', 'weights.18-0.39.h5', 'weights.180-0.38.h5', 'weights.181-0.38.h5', 'weights.182-0.38.h5', 'weights.183-0.38.h5', 'weights.184-0.38.h5', 'weights.185-0.38.h5', 'weights.186-0.38.h5', 'weights.187-0.38.h5', 'weights.188-0.38.h5', 'weights.189-0.38.h5', 'weights.19-0.39.h5', 'weights.190-0.38.h5', 'weights.191-0.38.h5', 'weights.192-0.38.h5', 'weights.193-0.38.h5', 'weights.194-0.38.h5', 'weights.195-0.38.h5', 'weights.196-0.38.h5', 'weights.197-0.38.h5', 'weights.198-0.38.h5', 'weights.199-0.38.h5', 'weights.20-0.39.h5', 'weights.200-0.38.h5', 'weights.201-0.38.h5', 'weights.202-0.38.h5', 'weights.203-0.38.h5', 'weights.204-0.38.h5', 'weights.205-0.38.h5', 'weights.206-0.38.h5', 'weights.207-0.38.h5', 'weights.208-0.38.h5', 'weights.209-0.38.h5', 'weights.21-0.39.h5', 'weights.210-0.38.h5', 'weights.211-0.38.h5', 'weights.212-0.38.h5', 'weights.213-0.38.h5', 'weights.214-0.38.h5', 'weights.215-0.38.h5', 'weights.216-0.38.h5', 'weights.217-0.38.h5', 'weights.218-0.38.h5', 'weights.219-0.38.h5', 'weights.22-0.39.h5', 'weights.220-0.38.h5', 'weights.221-0.38.h5', 'weights.222-0.38.h5', 'weights.223-0.38.h5', 'weights.224-0.38.h5', 'weights.225-0.38.h5', 'weights.226-0.38.h5', 'weights.227-0.38.h5', 'weights.228-0.38.h5', 'weights.229-0.38.h5', 'weights.23-0.39.h5', 'weights.230-0.38.h5', 'weights.231-0.38.h5', 'weights.232-0.38.h5', 'weights.233-0.38.h5', 'weights.234-0.38.h5', 'weights.235-0.38.h5', 'weights.236-0.38.h5', 'weights.237-0.38.h5', 'weights.238-0.38.h5', 'weights.239-0.38.h5', 'weights.24-0.39.h5', 'weights.240-0.38.h5', 'weights.241-0.38.h5', 'weights.242-0.38.h5', 'weights.243-0.38.h5', 'weights.244-0.38.h5', 'weights.245-0.38.h5', 'weights.246-0.38.h5', 'weights.247-0.38.h5', 'weights.248-0.38.h5', 'weights.249-0.38.h5', 'weights.25-0.39.h5', 'weights.250-0.38.h5', 'weights.251-0.38.h5', 'weights.252-0.38.h5', 'weights.253-0.38.h5', 'weights.254-0.38.h5', 'weights.255-0.38.h5', 'weights.256-0.38.h5', 'weights.257-0.38.h5', 'weights.258-0.38.h5', 'weights.259-0.38.h5', 'weights.26-0.39.h5', 'weights.260-0.38.h5', 'weights.261-0.38.h5', 'weights.262-0.38.h5', 'weights.263-0.38.h5', 'weights.264-0.38.h5', 'weights.265-0.38.h5', 'weights.266-0.38.h5', 'weights.267-0.38.h5', 'weights.268-0.38.h5', 'weights.269-0.38.h5', 'weights.27-0.39.h5', 'weights.270-0.38.h5', 'weights.271-0.38.h5', 'weights.272-0.38.h5', 'weights.273-0.38.h5', 'weights.274-0.37.h5', 'weights.275-0.38.h5', 'weights.276-0.38.h5', 'weights.277-0.38.h5', 'weights.278-0.38.h5', 'weights.279-0.38.h5', 'weights.28-0.39.h5', 'weights.280-0.38.h5', 'weights.281-0.38.h5', 'weights.282-0.38.h5', 'weights.283-0.38.h5', 'weights.284-0.38.h5', 'weights.285-0.37.h5', 'weights.286-0.38.h5', 'weights.287-0.38.h5', 'weights.288-0.38.h5', 'weights.289-0.38.h5', 'weights.29-0.38.h5', 'weights.290-0.38.h5', 'weights.291-0.38.h5', 'weights.292-0.38.h5', 'weights.293-0.37.h5', 'weights.294-0.38.h5', 'weights.295-0.38.h5', 'weights.296-0.38.h5', 'weights.297-0.38.h5', 'weights.298-0.37.h5', 'weights.299-0.38.h5', 'weights.30-0.39.h5', 'weights.300-0.38.h5', 'weights.301-0.38.h5', 'weights.302-0.38.h5', 'weights.303-0.38.h5', 'weights.304-0.38.h5', 'weights.305-0.38.h5', 'weights.306-0.38.h5', 'weights.307-0.38.h5', 'weights.308-0.38.h5', 'weights.309-0.38.h5', 'weights.31-0.39.h5', 'weights.310-0.38.h5', 'weights.311-0.38.h5', 'weights.312-0.38.h5', 'weights.313-0.38.h5', 'weights.314-0.38.h5', 'weights.315-0.37.h5', 'weights.316-0.38.h5', 'weights.317-0.38.h5', 'weights.318-0.37.h5', 'weights.319-0.38.h5', 'weights.32-0.38.h5', 'weights.320-0.38.h5', 'weights.321-0.38.h5', 'weights.322-0.37.h5', 'weights.323-0.38.h5', 'weights.324-0.38.h5', 'weights.325-0.38.h5', 'weights.326-0.38.h5', 'weights.327-0.38.h5', 'weights.328-0.37.h5', 'weights.329-0.38.h5', 'weights.33-0.38.h5', 'weights.330-0.38.h5', 'weights.331-0.37.h5', 'weights.332-0.38.h5', 'weights.333-0.38.h5', 'weights.334-0.38.h5', 'weights.335-0.38.h5', 'weights.336-0.38.h5', 'weights.337-0.38.h5', 'weights.338-0.38.h5', 'weights.339-0.38.h5', 'weights.34-0.38.h5', 'weights.340-0.37.h5', 'weights.341-0.37.h5', 'weights.342-0.37.h5', 'weights.343-0.37.h5', 'weights.344-0.38.h5', 'weights.345-0.37.h5', 'weights.346-0.38.h5', 'weights.347-0.38.h5', 'weights.348-0.38.h5', 'weights.349-0.38.h5', 'weights.35-0.39.h5', 'weights.350-0.38.h5', 'weights.351-0.38.h5', 'weights.352-0.38.h5', 'weights.353-0.38.h5', 'weights.354-0.38.h5', 'weights.355-0.37.h5', 'weights.356-0.37.h5', 'weights.357-0.37.h5', 'weights.358-0.37.h5', 'weights.359-0.38.h5', 'weights.36-0.39.h5', 'weights.360-0.37.h5', 'weights.361-0.38.h5', 'weights.362-0.37.h5', 'weights.363-0.37.h5', 'weights.364-0.37.h5', 'weights.365-0.37.h5', 'weights.366-0.38.h5', 'weights.367-0.37.h5', 'weights.368-0.38.h5', 'weights.369-0.38.h5', 'weights.37-0.39.h5', 'weights.370-0.37.h5', 'weights.371-0.37.h5', 'weights.372-0.38.h5', 'weights.373-0.38.h5', 'weights.374-0.37.h5', 'weights.375-0.38.h5', 'weights.376-0.38.h5', 'weights.377-0.38.h5', 'weights.378-0.38.h5', 'weights.379-0.38.h5', 'weights.38-0.38.h5', 'weights.380-0.37.h5', 'weights.381-0.37.h5', 'weights.382-0.37.h5', 'weights.383-0.37.h5', 'weights.384-0.37.h5', 'weights.385-0.38.h5', 'weights.386-0.38.h5', 'weights.387-0.38.h5', 'weights.388-0.38.h5', 'weights.389-0.38.h5', 'weights.39-0.38.h5', 'weights.390-0.38.h5', 'weights.391-0.37.h5', 'weights.392-0.38.h5', 'weights.393-0.37.h5', 'weights.394-0.37.h5', 'weights.395-0.37.h5', 'weights.396-0.38.h5', 'weights.397-0.37.h5', 'weights.398-0.38.h5', 'weights.399-0.38.h5', 'weights.40-0.38.h5', 'weights.400-0.37.h5', 'weights.401-0.37.h5', 'weights.402-0.37.h5', 'weights.403-0.37.h5', 'weights.404-0.37.h5', 'weights.405-0.38.h5', 'weights.406-0.38.h5', 'weights.407-0.37.h5', 'weights.408-0.37.h5', 'weights.409-0.38.h5', 'weights.41-0.38.h5', 'weights.410-0.37.h5', 'weights.411-0.37.h5', 'weights.412-0.37.h5', 'weights.413-0.37.h5', 'weights.414-0.38.h5', 'weights.415-0.37.h5', 'weights.416-0.38.h5', 'weights.417-0.37.h5', 'weights.418-0.37.h5', 'weights.419-0.38.h5', 'weights.42-0.38.h5', 'weights.420-0.37.h5', 'weights.421-0.38.h5', 'weights.422-0.38.h5', 'weights.423-0.37.h5', 'weights.424-0.37.h5', 'weights.425-0.37.h5', 'weights.426-0.37.h5', 'weights.427-0.37.h5', 'weights.428-0.38.h5', 'weights.429-0.38.h5', 'weights.43-0.38.h5', 'weights.430-0.38.h5', 'weights.431-0.37.h5', 'weights.432-0.38.h5', 'weights.433-0.37.h5', 'weights.434-0.37.h5', 'weights.435-0.38.h5', 'weights.436-0.37.h5', 'weights.437-0.37.h5', 'weights.438-0.37.h5', 'weights.439-0.37.h5', 'weights.44-0.38.h5', 'weights.440-0.37.h5', 'weights.441-0.37.h5', 'weights.442-0.37.h5', 'weights.443-0.38.h5', 'weights.444-0.38.h5', 'weights.445-0.38.h5', 'weights.446-0.37.h5', 'weights.447-0.38.h5', 'weights.448-0.37.h5', 'weights.449-0.37.h5', 'weights.45-0.38.h5', 'weights.450-0.38.h5', 'weights.451-0.37.h5', 'weights.452-0.37.h5', 'weights.453-0.37.h5', 'weights.454-0.37.h5', 'weights.455-0.37.h5', 'weights.456-0.38.h5', 'weights.457-0.38.h5', 'weights.458-0.37.h5', 'weights.459-0.37.h5', 'weights.46-0.38.h5', 'weights.460-0.37.h5', 'weights.461-0.38.h5', 'weights.462-0.37.h5', 'weights.463-0.37.h5', 'weights.464-0.37.h5', 'weights.465-0.38.h5', 'weights.466-0.37.h5', 'weights.467-0.37.h5', 'weights.468-0.37.h5', 'weights.469-0.38.h5', 'weights.47-0.38.h5', 'weights.470-0.37.h5', 'weights.471-0.38.h5', 'weights.472-0.37.h5', 'weights.473-0.37.h5', 'weights.474-0.38.h5', 'weights.475-0.37.h5', 'weights.476-0.37.h5', 'weights.477-0.37.h5', 'weights.478-0.37.h5', 'weights.479-0.37.h5', 'weights.48-0.38.h5', 'weights.480-0.37.h5', 'weights.481-0.38.h5', 'weights.482-0.38.h5', 'weights.483-0.37.h5', 'weights.484-0.37.h5', 'weights.485-0.37.h5', 'weights.486-0.37.h5', 'weights.487-0.38.h5', 'weights.488-0.37.h5', 'weights.489-0.38.h5', 'weights.49-0.39.h5', 'weights.490-0.37.h5', 'weights.491-0.37.h5', 'weights.492-0.37.h5', 'weights.493-0.37.h5', 'weights.494-0.37.h5', 'weights.495-0.37.h5', 'weights.496-0.38.h5', 'weights.497-0.37.h5', 'weights.498-0.37.h5', 'weights.499-0.37.h5', 'weights.50-0.38.h5', 'weights.500-0.37.h5', 'weights.501-0.38.h5', 'weights.502-0.38.h5', 'weights.503-0.37.h5', 'weights.504-0.37.h5', 'weights.505-0.37.h5', 'weights.506-0.37.h5', 'weights.507-0.37.h5', 'weights.508-0.37.h5', 'weights.509-0.37.h5', 'weights.51-0.38.h5', 'weights.510-0.37.h5', 'weights.511-0.37.h5', 'weights.512-0.37.h5', 'weights.513-0.37.h5', 'weights.514-0.37.h5', 'weights.515-0.37.h5', 'weights.516-0.37.h5', 'weights.517-0.37.h5', 'weights.518-0.37.h5', 'weights.519-0.37.h5', 'weights.52-0.38.h5', 'weights.520-0.37.h5', 'weights.521-0.37.h5', 'weights.522-0.37.h5', 'weights.523-0.37.h5', 'weights.524-0.37.h5', 'weights.525-0.37.h5', 'weights.526-0.37.h5', 'weights.527-0.37.h5', 'weights.528-0.37.h5', 'weights.529-0.37.h5', 'weights.53-0.38.h5', 'weights.530-0.37.h5', 'weights.531-0.37.h5', 'weights.532-0.37.h5', 'weights.533-0.37.h5', 'weights.534-0.38.h5', 'weights.535-0.37.h5', 'weights.536-0.37.h5', 'weights.537-0.37.h5', 'weights.538-0.37.h5', 'weights.539-0.37.h5', 'weights.54-0.38.h5', 'weights.540-0.37.h5', 'weights.541-0.37.h5', 'weights.542-0.37.h5', 'weights.543-0.37.h5', 'weights.544-0.37.h5', 'weights.545-0.37.h5', 'weights.546-0.37.h5', 'weights.547-0.37.h5', 'weights.548-0.37.h5', 'weights.549-0.38.h5', 'weights.55-0.38.h5', 'weights.550-0.38.h5', 'weights.551-0.37.h5', 'weights.552-0.37.h5', 'weights.553-0.38.h5', 'weights.554-0.37.h5', 'weights.555-0.37.h5', 'weights.556-0.37.h5', 'weights.557-0.37.h5', 'weights.558-0.37.h5', 'weights.559-0.37.h5', 'weights.56-0.38.h5', 'weights.560-0.37.h5', 'weights.561-0.37.h5', 'weights.562-0.37.h5', 'weights.563-0.37.h5', 'weights.564-0.37.h5', 'weights.565-0.37.h5', 'weights.566-0.37.h5', 'weights.567-0.37.h5', 'weights.568-0.37.h5', 'weights.569-0.37.h5', 'weights.57-0.38.h5', 'weights.570-0.37.h5', 'weights.571-0.38.h5', 'weights.572-0.37.h5', 'weights.573-0.37.h5', 'weights.574-0.37.h5', 'weights.575-0.37.h5', 'weights.576-0.38.h5', 'weights.577-0.37.h5', 'weights.578-0.37.h5', 'weights.579-0.38.h5', 'weights.58-0.38.h5', 'weights.580-0.37.h5', 'weights.581-0.37.h5', 'weights.582-0.37.h5', 'weights.583-0.37.h5', 'weights.584-0.37.h5', 'weights.585-0.37.h5', 'weights.586-0.37.h5', 'weights.587-0.37.h5', 'weights.588-0.37.h5', 'weights.589-0.38.h5', 'weights.59-0.38.h5', 'weights.590-0.37.h5', 'weights.591-0.37.h5', 'weights.592-0.37.h5', 'weights.593-0.37.h5', 'weights.594-0.37.h5', 'weights.595-0.37.h5', 'weights.596-0.37.h5', 'weights.597-0.37.h5', 'weights.598-0.37.h5', 'weights.599-0.37.h5', 'weights.60-0.38.h5', 'weights.600-0.37.h5', 'weights.601-0.37.h5', 'weights.602-0.37.h5', 'weights.603-0.37.h5', 'weights.604-0.38.h5', 'weights.605-0.37.h5', 'weights.606-0.37.h5', 'weights.607-0.37.h5', 'weights.608-0.37.h5', 'weights.609-0.38.h5', 'weights.61-0.38.h5', 'weights.610-0.37.h5', 'weights.611-0.37.h5', 'weights.612-0.37.h5', 'weights.613-0.37.h5', 'weights.614-0.37.h5', 'weights.615-0.37.h5', 'weights.616-0.37.h5', 'weights.617-0.37.h5', 'weights.618-0.37.h5', 'weights.619-0.37.h5', 'weights.62-0.38.h5', 'weights.620-0.37.h5', 'weights.621-0.37.h5', 'weights.622-0.37.h5', 'weights.623-0.37.h5', 'weights.624-0.37.h5', 'weights.625-0.37.h5', 'weights.626-0.37.h5', 'weights.627-0.37.h5', 'weights.628-0.37.h5', 'weights.629-0.37.h5', 'weights.63-0.38.h5', 'weights.630-0.37.h5', 'weights.631-0.37.h5', 'weights.632-0.37.h5', 'weights.633-0.37.h5', 'weights.634-0.37.h5', 'weights.635-0.37.h5', 'weights.636-0.37.h5', 'weights.637-0.37.h5', 'weights.638-0.37.h5', 'weights.639-0.37.h5', 'weights.64-0.38.h5', 'weights.640-0.37.h5', 'weights.641-0.37.h5', 'weights.642-0.37.h5', 'weights.643-0.37.h5', 'weights.644-0.37.h5', 'weights.645-0.37.h5', 'weights.646-0.37.h5', 'weights.647-0.37.h5', 'weights.648-0.37.h5', 'weights.649-0.37.h5', 'weights.65-0.38.h5', 'weights.650-0.37.h5', 'weights.651-0.37.h5', 'weights.652-0.37.h5', 'weights.653-0.37.h5', 'weights.654-0.37.h5', 'weights.655-0.37.h5', 'weights.656-0.37.h5', 'weights.657-0.37.h5', 'weights.658-0.37.h5', 'weights.659-0.37.h5', 'weights.66-0.38.h5', 'weights.660-0.37.h5', 'weights.661-0.37.h5', 'weights.662-0.37.h5', 'weights.663-0.37.h5', 'weights.664-0.37.h5', 'weights.665-0.37.h5', 'weights.666-0.37.h5', 'weights.667-0.38.h5', 'weights.668-0.37.h5', 'weights.669-0.37.h5', 'weights.67-0.38.h5', 'weights.670-0.37.h5', 'weights.671-0.37.h5', 'weights.672-0.37.h5', 'weights.673-0.37.h5', 'weights.674-0.37.h5', 'weights.675-0.37.h5', 'weights.676-0.37.h5', 'weights.677-0.37.h5', 'weights.678-0.37.h5', 'weights.679-0.37.h5', 'weights.68-0.38.h5', 'weights.680-0.37.h5', 'weights.681-0.38.h5', 'weights.682-0.37.h5', 'weights.683-0.37.h5', 'weights.684-0.37.h5', 'weights.685-0.37.h5', 'weights.686-0.37.h5', 'weights.687-0.37.h5', 'weights.688-0.37.h5', 'weights.689-0.37.h5', 'weights.69-0.38.h5', 'weights.690-0.37.h5', 'weights.691-0.37.h5', 'weights.692-0.37.h5', 'weights.693-0.37.h5', 'weights.694-0.37.h5', 'weights.695-0.37.h5', 'weights.696-0.37.h5', 'weights.697-0.37.h5', 'weights.698-0.37.h5', 'weights.699-0.37.h5', 'weights.70-0.38.h5', 'weights.700-0.37.h5', 'weights.701-0.37.h5', 'weights.702-0.37.h5', 'weights.703-0.38.h5', 'weights.704-0.37.h5', 'weights.705-0.37.h5', 'weights.706-0.37.h5', 'weights.707-0.37.h5', 'weights.708-0.38.h5', 'weights.709-0.37.h5', 'weights.71-0.38.h5', 'weights.710-0.37.h5', 'weights.711-0.37.h5', 'weights.712-0.37.h5', 'weights.713-0.37.h5', 'weights.714-0.37.h5', 'weights.715-0.37.h5', 'weights.716-0.37.h5', 'weights.717-0.38.h5', 'weights.718-0.37.h5', 'weights.719-0.37.h5', 'weights.72-0.38.h5', 'weights.720-0.37.h5', 'weights.721-0.37.h5', 'weights.722-0.37.h5', 'weights.723-0.37.h5', 'weights.724-0.37.h5', 'weights.725-0.37.h5', 'weights.726-0.37.h5', 'weights.727-0.37.h5', 'weights.728-0.37.h5', 'weights.729-0.37.h5', 'weights.73-0.38.h5', 'weights.730-0.37.h5', 'weights.731-0.37.h5', 'weights.732-0.37.h5', 'weights.733-0.37.h5', 'weights.734-0.37.h5', 'weights.735-0.37.h5', 'weights.736-0.37.h5', 'weights.737-0.37.h5', 'weights.738-0.37.h5', 'weights.739-0.37.h5', 'weights.74-0.38.h5', 'weights.740-0.37.h5', 'weights.741-0.37.h5', 'weights.742-0.37.h5', 'weights.743-0.37.h5', 'weights.744-0.37.h5', 'weights.745-0.37.h5', 'weights.746-0.37.h5', 'weights.747-0.38.h5', 'weights.748-0.37.h5', 'weights.749-0.37.h5', 'weights.75-0.38.h5', 'weights.750-0.37.h5', 'weights.751-0.37.h5', 'weights.752-0.37.h5', 'weights.753-0.37.h5', 'weights.754-0.38.h5', 'weights.755-0.37.h5', 'weights.756-0.37.h5', 'weights.757-0.37.h5', 'weights.758-0.37.h5', 'weights.759-0.37.h5', 'weights.76-0.38.h5', 'weights.760-0.37.h5', 'weights.761-0.37.h5', 'weights.762-0.37.h5', 'weights.763-0.37.h5', 'weights.764-0.37.h5', 'weights.765-0.37.h5', 'weights.766-0.37.h5', 'weights.767-0.37.h5', 'weights.768-0.37.h5', 'weights.769-0.37.h5', 'weights.77-0.38.h5', 'weights.770-0.37.h5', 'weights.771-0.37.h5', 'weights.772-0.37.h5', 'weights.773-0.37.h5', 'weights.774-0.37.h5', 'weights.775-0.37.h5', 'weights.776-0.37.h5', 'weights.777-0.37.h5', 'weights.778-0.37.h5', 'weights.779-0.37.h5', 'weights.78-0.38.h5', 'weights.780-0.37.h5', 'weights.781-0.37.h5', 'weights.782-0.37.h5', 'weights.783-0.37.h5', 'weights.784-0.37.h5', 'weights.785-0.37.h5', 'weights.786-0.37.h5', 'weights.787-0.37.h5', 'weights.788-0.37.h5', 'weights.789-0.37.h5', 'weights.79-0.38.h5', 'weights.790-0.37.h5', 'weights.791-0.37.h5', 'weights.792-0.37.h5', 'weights.793-0.37.h5', 'weights.794-0.37.h5', 'weights.795-0.37.h5', 'weights.796-0.37.h5', 'weights.797-0.37.h5', 'weights.798-0.37.h5', 'weights.799-0.37.h5', 'weights.80-0.38.h5', 'weights.800-0.37.h5', 'weights.801-0.37.h5', 'weights.802-0.37.h5', 'weights.803-0.37.h5', 'weights.804-0.37.h5', 'weights.805-0.37.h5', 'weights.806-0.37.h5', 'weights.807-0.37.h5', 'weights.808-0.37.h5', 'weights.809-0.37.h5', 'weights.81-0.38.h5', 'weights.810-0.37.h5', 'weights.811-0.37.h5', 'weights.812-0.37.h5', 'weights.813-0.37.h5', 'weights.814-0.37.h5', 'weights.815-0.37.h5', 'weights.816-0.37.h5', 'weights.817-0.37.h5', 'weights.818-0.37.h5', 'weights.819-0.37.h5', 'weights.82-0.38.h5', 'weights.820-0.37.h5', 'weights.821-0.37.h5', 'weights.822-0.37.h5', 'weights.823-0.37.h5', 'weights.824-0.37.h5', 'weights.825-0.37.h5', 'weights.826-0.37.h5', 'weights.827-0.37.h5', 'weights.828-0.37.h5', 'weights.829-0.37.h5', 'weights.83-0.38.h5', 'weights.830-0.37.h5', 'weights.831-0.37.h5', 'weights.832-0.38.h5', 'weights.833-0.37.h5', 'weights.834-0.37.h5', 'weights.835-0.37.h5', 'weights.836-0.37.h5', 'weights.837-0.37.h5', 'weights.838-0.37.h5', 'weights.839-0.37.h5', 'weights.84-0.38.h5', 'weights.840-0.37.h5', 'weights.841-0.37.h5', 'weights.842-0.37.h5', 'weights.843-0.37.h5', 'weights.844-0.37.h5', 'weights.845-0.37.h5', 'weights.846-0.37.h5', 'weights.847-0.37.h5', 'weights.848-0.37.h5', 'weights.849-0.37.h5', 'weights.85-0.38.h5', 'weights.850-0.37.h5', 'weights.851-0.37.h5', 'weights.852-0.37.h5', 'weights.853-0.37.h5', 'weights.854-0.37.h5', 'weights.855-0.37.h5', 'weights.856-0.37.h5', 'weights.857-0.37.h5', 'weights.858-0.37.h5', 'weights.859-0.37.h5', 'weights.86-0.38.h5', 'weights.860-0.37.h5', 'weights.861-0.37.h5', 'weights.862-0.37.h5', 'weights.863-0.37.h5', 'weights.864-0.37.h5', 'weights.865-0.37.h5', 'weights.866-0.37.h5', 'weights.867-0.37.h5', 'weights.868-0.37.h5', 'weights.869-0.37.h5', 'weights.87-0.38.h5', 'weights.870-0.37.h5', 'weights.871-0.37.h5', 'weights.872-0.37.h5', 'weights.873-0.37.h5', 'weights.874-0.37.h5', 'weights.875-0.37.h5', 'weights.876-0.37.h5', 'weights.877-0.37.h5', 'weights.878-0.37.h5', 'weights.879-0.37.h5', 'weights.88-0.38.h5', 'weights.880-0.37.h5', 'weights.881-0.37.h5', 'weights.882-0.37.h5', 'weights.883-0.37.h5', 'weights.884-0.37.h5', 'weights.885-0.37.h5', 'weights.886-0.37.h5', 'weights.887-0.37.h5', 'weights.888-0.37.h5', 'weights.889-0.37.h5', 'weights.89-0.38.h5', 'weights.890-0.37.h5', 'weights.891-0.37.h5', 'weights.892-0.37.h5', 'weights.893-0.37.h5', 'weights.894-0.37.h5', 'weights.895-0.37.h5', 'weights.896-0.37.h5', 'weights.897-0.37.h5', 'weights.898-0.37.h5', 'weights.899-0.37.h5', 'weights.90-0.38.h5', 'weights.900-0.37.h5', 'weights.901-0.38.h5', 'weights.902-0.37.h5', 'weights.903-0.37.h5', 'weights.904-0.37.h5', 'weights.905-0.37.h5', 'weights.906-0.37.h5', 'weights.907-0.37.h5', 'weights.908-0.37.h5', 'weights.909-0.37.h5', 'weights.91-0.38.h5', 'weights.910-0.37.h5', 'weights.911-0.37.h5', 'weights.912-0.37.h5', 'weights.913-0.37.h5', 'weights.914-0.38.h5', 'weights.915-0.37.h5', 'weights.916-0.37.h5', 'weights.917-0.37.h5', 'weights.918-0.37.h5', 'weights.919-0.37.h5', 'weights.92-0.38.h5', 'weights.920-0.37.h5', 'weights.921-0.37.h5', 'weights.922-0.37.h5', 'weights.923-0.37.h5', 'weights.924-0.37.h5', 'weights.925-0.37.h5', 'weights.926-0.37.h5', 'weights.927-0.37.h5', 'weights.928-0.37.h5', 'weights.929-0.37.h5', 'weights.93-0.38.h5', 'weights.930-0.37.h5', 'weights.931-0.37.h5', 'weights.932-0.37.h5', 'weights.933-0.37.h5', 'weights.934-0.37.h5', 'weights.935-0.37.h5', 'weights.936-0.37.h5', 'weights.937-0.37.h5', 'weights.938-0.37.h5', 'weights.939-0.37.h5', 'weights.94-0.38.h5', 'weights.940-0.37.h5', 'weights.941-0.37.h5', 'weights.942-0.37.h5', 'weights.943-0.37.h5', 'weights.944-0.37.h5', 'weights.945-0.37.h5', 'weights.946-0.37.h5', 'weights.947-0.37.h5', 'weights.948-0.37.h5', 'weights.949-0.37.h5', 'weights.95-0.38.h5', 'weights.950-0.37.h5', 'weights.951-0.37.h5', 'weights.952-0.37.h5', 'weights.953-0.37.h5', 'weights.954-0.37.h5', 'weights.955-0.37.h5', 'weights.956-0.37.h5', 'weights.957-0.37.h5', 'weights.958-0.37.h5', 'weights.959-0.37.h5', 'weights.96-0.38.h5', 'weights.960-0.37.h5', 'weights.961-0.37.h5', 'weights.962-0.37.h5', 'weights.963-0.37.h5', 'weights.964-0.37.h5', 'weights.965-0.37.h5', 'weights.966-0.37.h5', 'weights.967-0.37.h5', 'weights.968-0.37.h5', 'weights.969-0.37.h5', 'weights.97-0.38.h5', 'weights.970-0.37.h5', 'weights.971-0.37.h5', 'weights.972-0.37.h5', 'weights.973-0.37.h5', 'weights.974-0.37.h5', 'weights.975-0.37.h5', 'weights.976-0.37.h5', 'weights.977-0.37.h5', 'weights.978-0.37.h5', 'weights.979-0.37.h5', 'weights.98-0.38.h5', 'weights.980-0.37.h5', 'weights.981-0.37.h5', 'weights.982-0.37.h5', 'weights.983-0.37.h5', 'weights.984-0.37.h5', 'weights.985-0.37.h5', 'weights.986-0.37.h5', 'weights.987-0.37.h5', 'weights.988-0.37.h5', 'weights.989-0.37.h5', 'weights.99-0.38.h5', 'weights.990-0.37.h5', 'weights.991-0.37.h5', 'weights.992-0.37.h5', 'weights.993-0.37.h5', 'weights.994-0.37.h5', 'weights.995-0.37.h5', 'weights.996-0.37.h5', 'weights.997-0.37.h5', 'weights.998-0.37.h5', 'weights.999-0.37.h5']\n"
     ]
    }
   ],
   "source": [
    "dirname = \"/Users/mykola/Google Drive/NNmodels/1000_epochs/\"\n",
    "#dirname = \"/Users/mykola/MLHEP/MLHEP-2020-muon-id/NNmodels\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(dirname) if isfile(join(dirname, f))]\n",
    "print (sorted(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  weights.559-4.03.h5  score:  0.6122303863474757\n",
      "model:  weights.132-3.93.h5  score:  0.6286797819388481\n",
      "model:  weights.988-4.07.h5  score:  0.6014221379473809\n",
      "model:  weights.819-4.05.h5  score:  0.6133206921071344\n",
      "model:  weights.294-3.98.h5  score:  0.5844986963735482\n",
      "model:  weights.284-3.98.h5  score:  0.5989570988385874\n",
      "model:  weights.747-4.03.h5  score:  0.6042664138421426\n",
      "model:  weights.108-3.94.h5  score:  0.606257406968476\n",
      "model:  weights.794-4.06.h5  score:  0.6164019909931263\n",
      "model:  weights.118-3.94.h5  score:  0.6131784783123964\n",
      "model:  weights.923-4.08.h5  score:  0.6210950462194833\n",
      "model:  weights.287-3.96.h5  score:  0.6020383977245792\n",
      "model:  weights.509-4.01.h5  score:  0.5961602275420715\n",
      "model:  weights.420-4.02.h5  score:  0.5978193884806826\n",
      "model:  weights.999-4.06.h5  score:  0.6101445840246504\n",
      "model:  weights.286-3.97.h5  score:  0.6133680967053804\n",
      "model:  weights.119-3.95.h5  score:  0.5822706802559848\n",
      "model:  weights.859-4.07.h5  score:  0.6033657264754682\n",
      "model:  weights.498-4.03.h5  score:  0.6110452713913249\n",
      "model:  weights.912-4.04.h5  score:  0.6122303863474757\n",
      "model:  weights.724-4.02.h5  score:  0.6052145058070633\n",
      "model:  weights.964-4.12.h5  score:  0.6026072529035317\n",
      "model:  weights.396-4.00.h5  score:  0.6144109978667931\n",
      "model:  weights.938-4.03.h5  score:  0.6167812277790946\n",
      "model:  weights.883-4.06.h5  score:  0.6159753496089121\n",
      "model:  weights.96-3.94.h5  score:  0.6180137473334913\n",
      "model:  weights.735-4.03.h5  score:  0.5923678596823891\n",
      "model:  weights.606-3.97.h5  score:  0.6189144347001659\n",
      "model:  weights.913-4.05.h5  score:  0.6040293908509126\n",
      "model:  weights.336-3.99.h5  score:  0.6179663427352453\n",
      "model:  weights.892-4.07.h5  score:  0.6052619104053093\n",
      "model:  weights.881-4.09.h5  score:  0.6063996207632141\n",
      "model:  weights.573-3.99.h5  score:  0.5919412182981749\n",
      "model:  weights.563-3.99.h5  score:  0.6186300071106897\n",
      "model:  weights.335-3.97.h5  score:  0.6247926048826736\n",
      "model:  weights.183-3.96.h5  score:  0.588859919412183\n",
      "model:  weights.693-4.04.h5  score:  0.6106186300071107\n",
      "model:  weights.779-4.02.h5  score:  0.6090068736667457\n",
      "model:  weights.611-4.02.h5  score:  0.6069684759421664\n",
      "model:  weights.769-4.02.h5  score:  0.6090542782649917\n",
      "model:  weights.23-3.97.h5  score:  0.6215216876036975\n",
      "model:  weights.577-4.02.h5  score:  0.6089594690684996\n",
      "model:  weights.62-3.94.h5  score:  0.6049774828158332\n",
      "model:  weights.20-3.99.h5  score:  0.6217113059966817\n",
      "model:  weights.363-4.01.h5  score:  0.5996681678122778\n",
      "model:  weights.240-3.95.h5  score:  0.6135577150983645\n",
      "model:  weights.447-3.98.h5  score:  0.6147902346527613\n",
      "model:  weights.712-4.06.h5  score:  0.6097179426404361\n",
      "model:  weights.201-3.96.h5  score:  0.6054515287982934\n",
      "model:  weights.63-3.95.h5  score:  0.6286323773406021\n",
      "model:  weights.32-3.96.h5  score:  0.6165916093861105\n",
      "model:  weights.566-4.03.h5  score:  0.6132258829106423\n",
      "model:  weights.781-4.01.h5  score:  0.5980564114719128\n",
      "model:  weights.40-3.96.h5  score:  0.6021806115193173\n",
      "model:  weights.262-3.97.h5  score:  0.6069210713439204\n",
      "model:  weights.83-3.93.h5  score:  0.6487793315951648\n",
      "model:  weights.730-4.04.h5  score:  0.5987200758473572\n",
      "model:  weights.223-3.94.h5  score:  0.6029390850912538\n",
      "model:  weights.155-3.94.h5  score:  0.6405783360986016\n",
      "model:  weights.145-3.94.h5  score:  0.6224223749703721\n",
      "model:  weights.770-4.06.h5  score:  0.6180137473334913\n",
      "model:  weights.554-4.01.h5  score:  0.6070632851386585\n",
      "model:  weights.515-4.02.h5  score:  0.5871533538753259\n",
      "model:  weights.663-4.02.h5  score:  0.5933159516473098\n",
      "model:  weights.484-4.00.h5  score:  0.6091016828632377\n",
      "model:  weights.799-4.04.h5  score:  0.60322351268073\n",
      "model:  weights.873-4.04.h5  score:  0.5970135103105001\n",
      "model:  weights.386-3.99.h5  score:  0.5947854941929367\n",
      "model:  weights.120-3.98.h5  score:  0.5909931263332543\n",
      "model:  weights.172-3.95.h5  score:  0.6324721497985305\n",
      "model:  weights.756-4.06.h5  score:  0.5926522872718654\n",
      "model:  weights.366-4.02.h5  score:  0.6253140554633799\n",
      "model:  weights.572-4.01.h5  score:  0.6147902346527613\n",
      "model:  weights.394-3.96.h5  score:  0.6127992415264281\n",
      "model:  weights.518-4.04.h5  score:  0.6004266413842143\n",
      "model:  weights.07-4.05.h5  score:  0.46854704906375916\n",
      "model:  weights.163-3.94.h5  score:  0.6035553448684522\n",
      "model:  weights.654-4.03.h5  score:  0.6053567196018014\n",
      "model:  weights.872-4.05.h5  score:  0.6100023702299123\n",
      "model:  weights.615-4.00.h5  score:  0.6283479497511258\n",
      "model:  weights.76-3.96.h5  score:  0.6223749703721261\n",
      "model:  weights.326-4.00.h5  score:  0.6183455795212136\n",
      "model:  weights.985-4.11.h5  score:  0.6021806115193173\n",
      "model:  weights.254-3.97.h5  score:  0.619198862289642\n",
      "model:  weights.764-4.04.h5  score:  0.6070632851386585\n",
      "model:  weights.470-3.99.h5  score:  0.6282057359563877\n",
      "model:  weights.236-3.97.h5  score:  0.6259777198388243\n",
      "model:  weights.479-4.00.h5  score:  0.6164968001896184\n",
      "model:  weights.726-4.09.h5  score:  0.6083906138895473\n",
      "model:  weights.991-4.07.h5  score:  0.6029864896894999\n",
      "model:  weights.851-4.06.h5  score:  0.6\n",
      "model:  weights.636-4.03.h5  score:  0.611092675989571\n",
      "model:  weights.852-4.08.h5  score:  0.606352216164968\n",
      "model:  weights.111-3.94.h5  score:  0.6184403887177056\n",
      "model:  weights.842-4.08.h5  score:  0.6076795449158569\n",
      "model:  weights.44-3.94.h5  score:  0.5956861815596113\n",
      "model:  weights.980-4.06.h5  score:  0.6033183218772221\n",
      "model:  weights.345-4.01.h5  score:  0.5845461009717943\n",
      "model:  weights.592-4.07.h5  score:  0.6053093150035553\n",
      "model:  weights.993-4.08.h5  score:  0.5956861815596113\n",
      "model:  weights.16-3.99.h5  score:  0.5563403650154065\n",
      "model:  weights.227-3.96.h5  score:  0.623797108319507\n",
      "model:  weights.692-4.11.h5  score:  0.5929841194595876\n",
      "model:  weights.481-4.03.h5  score:  0.6287745911353402\n",
      "model:  weights.424-4.00.h5  score:  0.6029864896894999\n",
      "model:  weights.682-4.01.h5  score:  0.5983408390613889\n",
      "model:  weights.310-3.99.h5  score:  0.6164968001896184\n",
      "model:  weights.434-4.00.h5  score:  0.5988148850438493\n",
      "model:  weights.303-3.97.h5  score:  0.5921308366911591\n",
      "model:  weights.309-4.00.h5  score:  0.6006162597771983\n",
      "model:  weights.877-4.02.h5  score:  0.6145058070632852\n",
      "model:  weights.271-4.00.h5  score:  0.6130836691159043\n",
      "model:  weights.925-4.05.h5  score:  0.6108082484000948\n",
      "model:  weights.866-4.03.h5  score:  0.6129414553211662\n",
      "model:  weights.308-4.01.h5  score:  0.5950225171841669\n",
      "model:  weights.683-4.00.h5  score:  0.6065892391561981\n",
      "model:  weights.753-4.01.h5  score:  0.5983882436596349\n",
      "model:  weights.965-4.07.h5  score:  0.5966342735245319\n",
      "model:  weights.975-4.07.h5  score:  0.6102393932211424\n",
      "model:  weights.780-4.04.h5  score:  0.5843090779805641\n",
      "model:  weights.903-4.11.h5  score:  0.6145058070632852\n",
      "model:  weights.632-3.98.h5  score:  0.611187485186063\n",
      "model:  weights.914-4.09.h5  score:  0.6043138184403887\n",
      "model:  weights.946-4.04.h5  score:  0.6062100023702299\n",
      "model:  weights.416-4.02.h5  score:  0.6044086276368807\n",
      "model:  weights.845-4.00.h5  score:  0.6176345105475232\n",
      "model:  weights.814-4.03.h5  score:  0.6129888599194122\n",
      "model:  weights.08-4.08.h5  score:  0.4988859919412183\n",
      "model:  weights.915-4.08.h5  score:  0.6132732875088883\n",
      "model:  weights.209-3.97.h5  score:  0.619009243896658\n",
      "model:  weights.446-4.00.h5  score:  0.6124200047404599\n",
      "model:  weights.456-4.00.h5  score:  0.6199099312633325\n",
      "model:  weights.162-3.94.h5  score:  0.5939322114245081\n",
      "model:  weights.403-3.99.h5  score:  0.5990519080350794\n",
      "model:  weights.921-4.02.h5  score:  0.6240341313107372\n",
      "model:  weights.795-4.02.h5  score:  0.6025124437070396\n",
      "model:  weights.785-4.02.h5  score:  0.6024650391087936\n",
      "model:  weights.562-4.00.h5  score:  0.6152642806352216\n",
      "model:  weights.820-4.09.h5  score:  0.6092913012562219\n",
      "model:  weights.833-4.07.h5  score:  0.612751836928182\n",
      "model:  weights.605-4.01.h5  score:  0.6135577150983645\n",
      "model:  weights.387-3.99.h5  score:  0.61588054041242\n",
      "model:  weights.411-3.96.h5  score:  0.6134629059018725\n",
      "model:  weights.532-4.02.h5  score:  0.6214268784072055\n",
      "model:  weights.37-3.94.h5  score:  0.5966342735245319\n",
      "model:  weights.412-3.98.h5  score:  0.6036975586631903\n",
      "model:  weights.757-4.06.h5  score:  0.5683811329698981\n",
      "model:  weights.402-3.98.h5  score:  0.6226593979616023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  weights.888-4.08.h5  score:  0.6036027494666982\n",
      "model:  weights.226-3.96.h5  score:  0.6214742830054515\n",
      "model:  weights.952-4.03.h5  score:  0.6123726001422138\n",
      "model:  weights.942-4.03.h5  score:  0.6149798530457454\n",
      "model:  weights.45-3.94.h5  score:  0.5844038871770562\n",
      "model:  weights.501-4.01.h5  score:  0.6165442047878644\n",
      "model:  weights.469-4.01.h5  score:  0.6263569566247926\n",
      "model:  weights.55-3.94.h5  score:  0.6128940507229201\n",
      "model:  weights.626-4.02.h5  score:  0.6203839772457929\n",
      "model:  weights.677-4.01.h5  score:  0.5980090068736668\n",
      "model:  weights.101-3.95.h5  score:  0.6303863474757051\n",
      "model:  weights.54-3.95.h5  score:  0.6074899265228727\n",
      "model:  weights.742-4.11.h5  score:  0.6161649680018961\n",
      "model:  weights.461-3.99.h5  score:  0.6346053567196018\n",
      "model:  weights.582-4.06.h5  score:  0.5902346527613178\n",
      "model:  weights.100-3.94.h5  score:  0.6336572647546812\n",
      "model:  weights.413-4.04.h5  score:  0.6118037449632614\n",
      "model:  weights.475-4.02.h5  score:  0.6041242000474046\n",
      "model:  weights.278-3.98.h5  score:  0.6565536857075136\n",
      "model:  weights.465-4.02.h5  score:  0.5943114482104764\n",
      "model:  weights.313-3.96.h5  score:  0.5947380895946907\n",
      "model:  weights.837-4.01.h5  score:  0.6256458876511022\n",
      "model:  weights.312-3.97.h5  score:  0.5849253377577625\n",
      "model:  weights.738-4.04.h5  score:  0.6003318321877222\n",
      "model:  weights.464-4.03.h5  score:  0.6051671012088172\n",
      "model:  weights.269-3.99.h5  score:  0.6011377103579048\n",
      "model:  weights.425-4.00.h5  score:  0.6010429011614127\n",
      "model:  weights.13-4.02.h5  score:  0.5592320455084143\n",
      "model:  weights.934-4.05.h5  score:  0.6152168760369756\n",
      "model:  weights.536-4.04.h5  score:  0.6093387058544679\n",
      "model:  weights.526-4.04.h5  score:  0.6090542782649917\n",
      "model:  weights.659-4.00.h5  score:  0.5899502251718417\n",
      "model:  weights.608-4.03.h5  score:  0.601659160938611\n",
      "model:  weights.105-3.93.h5  score:  0.625361460061626\n",
      "model:  weights.197-3.95.h5  score:  0.604787864422849\n",
      "model:  weights.212-4.00.h5  score:  0.6081061863000711\n",
      "model:  weights.218-3.97.h5  score:  0.6133206921071344\n",
      "model:  weights.29-3.98.h5  score:  0.5545863948803034\n",
      "model:  weights.771-4.02.h5  score:  0.607869163308841\n",
      "model:  weights.905-4.09.h5  score:  0.6112348897843091\n",
      "model:  weights.186-3.94.h5  score:  0.6105238208106186\n",
      "model:  weights.219-3.96.h5  score:  0.617397487556293\n",
      "model:  weights.361-3.96.h5  score:  0.6134629059018725\n",
      "model:  weights.196-3.94.h5  score:  0.6228016117563404\n",
      "model:  weights.372-3.98.h5  score:  0.6148376392510073\n",
      "model:  weights.549-4.02.h5  score:  0.6039345816544205\n",
      "model:  weights.17-4.00.h5  score:  0.5954491585683811\n",
      "model:  weights.961-4.04.h5  score:  0.6050248874140791\n",
      "model:  weights.809-4.04.h5  score:  0.5957809907561034\n",
      "model:  weights.933-4.09.h5  score:  0.6135103105001185\n",
      "model:  weights.471-4.00.h5  score:  0.6259303152405783\n",
      "model:  weights.355-3.99.h5  score:  0.6004266413842143\n",
      "model:  weights.1000-4.06.h5  score:  0.6090068736667457\n",
      "model:  weights.468-3.99.h5  score:  0.6268310026072529\n",
      "model:  weights.960-4.05.h5  score:  0.6077269495141029\n",
      "model:  weights.931-4.06.h5  score:  0.6109978667930789\n",
      "model:  weights.818-4.05.h5  score:  0.6063996207632141\n",
      "model:  weights.943-4.06.h5  score:  0.6060677885754918\n",
      "model:  weights.87-3.94.h5  score:  0.6438966579758236\n",
      "model:  weights.500-4.04.h5  score:  0.5923678596823891\n",
      "model:  weights.478-4.04.h5  score:  0.6082484000948092\n",
      "model:  weights.182-3.96.h5  score:  0.5996681678122778\n",
      "model:  weights.03-4.22.h5  score:  0.4002844275894762\n",
      "model:  weights.850-4.02.h5  score:  0.5990045034368334\n",
      "model:  weights.192-3.96.h5  score:  0.5971557241052382\n",
      "model:  weights.452-4.03.h5  score:  0.596918701114008\n",
      "model:  weights.924-4.11.h5  score:  0.6010903057596587\n",
      "model:  weights.725-4.02.h5  score:  0.596681678122778\n",
      "model:  weights.882-4.06.h5  score:  0.6129888599194122\n",
      "model:  weights.126-3.94.h5  score:  0.6305759658686892\n",
      "model:  weights.136-3.94.h5  score:  0.6275420715809433\n",
      "model:  weights.649-3.99.h5  score:  0.6280635221616497\n",
      "model:  weights.640-4.00.h5  score:  0.6179189381369993\n",
      "model:  weights.72-3.95.h5  score:  0.6103342024176345\n",
      "model:  weights.379-3.97.h5  score:  0.6029864896894999\n",
      "model:  weights.768-4.02.h5  score:  0.5989096942403414\n",
      "model:  weights.641-4.01.h5  score:  0.6171604645650628\n",
      "model:  weights.729-4.01.h5  score:  0.615785731215928\n",
      "model:  weights.200-3.96.h5  score:  0.6339890969424035\n",
      "model:  weights.210-3.96.h5  score:  0.6141265702773169\n",
      "model:  weights.788-3.99.h5  score:  0.6272102393932212\n",
      "model:  weights.241-3.95.h5  score:  0.6414790234652761\n",
      "model:  weights.21-3.99.h5  score:  0.5849253377577625\n",
      "model:  weights.752-4.05.h5  score:  0.600047404598246\n",
      "model:  weights.765-4.10.h5  score:  0.5980090068736668\n",
      "model:  weights.527-4.01.h5  score:  0.5903294619578099\n",
      "model:  weights.537-4.01.h5  score:  0.6108082484000948\n",
      "model:  weights.994-4.05.h5  score:  0.5882910642332306\n",
      "model:  weights.319-3.98.h5  score:  0.6228964209528324\n",
      "model:  weights.50-3.97.h5  score:  0.6051671012088172\n",
      "model:  weights.619-4.06.h5  score:  0.5999051908035079\n",
      "model:  weights.906-4.03.h5  score:  0.6032709172789761\n",
      "model:  weights.887-4.01.h5  score:  0.6022754207158094\n",
      "model:  weights.815-4.07.h5  score:  0.6041716046456507\n",
      "model:  weights.485-4.00.h5  score:  0.6177293197440151\n",
      "model:  weights.273-3.97.h5  score:  0.6296752785020147\n",
      "model:  weights.587-4.05.h5  score:  0.6008058781701825\n",
      "model:  weights.270-3.99.h5  score:  0.6394406257406968\n",
      "model:  weights.855-4.05.h5  score:  0.5987674804456032\n",
      "model:  weights.622-4.00.h5  score:  0.6172552737615549\n",
      "model:  weights.812-4.04.h5  score:  0.6020858023228253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e3631c8c5b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/{dirname}/{mod}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'<lambda>'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"constrained_MSE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconstrained_MSE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprediction_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIMPLE_FEATURE_COLUMNS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFOI_Names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrejection90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"model: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" score: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorlayer as tl\n",
    "from tensorflow.python.keras import losses\n",
    "scores =[]\n",
    "for mod in onlyfiles:\n",
    "    if \"weights\" in mod:\n",
    "        model = tf.keras.models.load_model(f\"/{dirname}/{mod}\",custom_objects={'<lambda>': lambda x : tl.act.lrelu(x, 0.1),\"constrained_MSE\":constrained_MSE})\n",
    "        prediction_validation = np.array(model.predict(validation.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values),dtype=float)\n",
    "        scores.append(scoring.rejection90(validation.label, prediction_validation))\n",
    "        print (\"model: \",mod, \" score: \",scores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3630471, 73)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/mykola/MLHEP/MLHEP-2020-muon-id\"\n",
    "#DATA_PATH = dirname\n",
    "\n",
    "\n",
    "test = utils.load_full_test_csv(DATA_PATH, \"test-features.csv.gz\")\n",
    "\n",
    "test_closest_hits = test.apply(utils.find_closest_hit_per_station,axis=1)\n",
    "Xarray_test = np.stack(test_closest_hits)\n",
    "Closest_hits_test = pd.DataFrame(data=Xarray_test, index = range(len(test)), columns = FOI_Names)\n",
    "Closest_hits_test.to_csv(\"big_test_transformed_foi.csv\")\n",
    "\n",
    "#print (Closest_hits)\n",
    "nn_test_input = pd.concat([test, Closest_hits_test], axis=1, sort=False)\n",
    "print(nn_test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(os.path.join(DATA_PATH, \"test-features.csv.gz\"),index_col=\"id\", usecols=[\"id\"]+utils.SIMPLE_FEATURE_COLUMNS)\n",
    "foi_test = pd.read_csv(os.path.join(DATA_PATH, \"big_test_transformed_foi.csv\"))\n",
    "nn_test_input = pd.concat([test, foi_test], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ncl[0]  ncl[1]  ncl[2]  ncl[3]  avg_cs[0]  avg_cs[1]  avg_cs[2]  \\\n",
      "0            55      12      10      15   2.127273   1.166667   1.500000   \n",
      "1            43      18      12      15   2.697675   1.111111   1.333333   \n",
      "2            90      34      19      29   1.800000   2.058824   1.210526   \n",
      "3           141      43      15      16   4.609929   2.744186   1.466667   \n",
      "4            22      18       8       9   1.636364   2.444444   1.000000   \n",
      "...         ...     ...     ...     ...        ...        ...        ...   \n",
      "3630466      19       6      23       8   1.736842   1.333333   1.391304   \n",
      "3630467      42      10       6      18   2.309524   2.100000   2.166667   \n",
      "3630468      36       6       8      25   1.638889   1.666667   1.250000   \n",
      "3630469      90      31      10      15   2.888889   2.387097   1.300000   \n",
      "3630470      61      18       8      14   1.737705   1.388889   1.000000   \n",
      "\n",
      "         avg_cs[3]  ndof  MatchedHit_TYPE[0]  ...  closest_z_2  closest_z_3  \\\n",
      "0         1.600000     8                   2  ...    17597.812    18806.229   \n",
      "1         1.600000     8                   2  ...    17514.170    18722.213   \n",
      "2         1.344828     8                   2  ...    17508.303    18715.945   \n",
      "3         1.125000     8                   2  ...    17517.113    18926.592   \n",
      "4         1.555556     8                   2  ...    17600.200    18808.380   \n",
      "...            ...   ...                 ...  ...          ...          ...   \n",
      "3630466   1.125000     8                   2  ...    17720.508    19006.710   \n",
      "3630467   1.055556     8                   2  ...    17812.525    19021.870   \n",
      "3630468   1.400000     8                   2  ...    17524.674    18733.832   \n",
      "3630469   1.333333     8                   2  ...    17712.530    18920.395   \n",
      "3630470   1.000000     8                   1  ...    17719.980    18928.750   \n",
      "\n",
      "         closest_dx_0  closest_dx_1  closest_dx_2  closest_dx_3  closest_dy_0  \\\n",
      "0           12.750000     13.750000     59.000000     63.000000     63.078957   \n",
      "1            6.375000      6.875000     29.500000     31.500000     31.479162   \n",
      "2           25.500000     27.500000    118.000000    126.000000    126.278550   \n",
      "3            3.208333      3.458333     14.833333     31.500000     15.679264   \n",
      "4            3.208333      3.458333     14.833333     15.833333     15.679264   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "3630466      3.208333      3.458333     14.833333     15.833333     15.679264   \n",
      "3630467     12.750000     13.750000     59.000000     63.000000     63.078957   \n",
      "3630468      6.375000      6.875000     29.500000     31.500000     31.479162   \n",
      "3630469      6.375000      6.875000     29.500000     31.500000     31.479162   \n",
      "3630470      3.208333      3.458333     14.833333     15.833333    126.278550   \n",
      "\n",
      "         closest_dy_1  closest_dy_2  closest_dy_3  \n",
      "0           68.078926     73.078896     78.078860  \n",
      "1           33.979145     36.479130     38.979115  \n",
      "2          136.278490    146.278410    156.278350  \n",
      "3           16.929256     18.179249     38.979115  \n",
      "4           16.929256     18.179249     19.429240  \n",
      "...               ...           ...           ...  \n",
      "3630466     16.929256     18.179249     19.429240  \n",
      "3630467     68.078926     73.078896     78.078860  \n",
      "3630468     33.979145     36.479130     38.979115  \n",
      "3630469     33.979145     36.479130     38.979115  \n",
      "3630470     16.929256     18.179249     19.429240  \n",
      "\n",
      "[3630471 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "print (nn_test_input)\n",
    "nn_test_input.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names] = sc.transform(nn_test_input.loc[:, utils.SIMPLE_FEATURE_COLUMNS+FOI_Names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "[[0.89707601]\n",
      " [0.89613426]\n",
      " [0.96014762]\n",
      " ...\n",
      " [0.99611664]\n",
      " [0.88737953]\n",
      " [0.03571945]]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"weights.999-0.37.h5\"\n",
    "#  \n",
    "model = tf.keras.models.load_model(f\"{dirname}/{model_name}\",custom_objects={'<lambda>': lambda x : tl.act.lrelu(x, 0.1),})\n",
    "print (\"model loaded\")\n",
    "predictions = np.array(model.predict_proba(nn_test_input[utils.SIMPLE_FEATURE_COLUMNS+FOI_Names].values),dtype=float)\n",
    "print (predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89707601 0.89613426 0.96014762 ... 0.99611664 0.88737953 0.03571945]\n"
     ]
    }
   ],
   "source": [
    "predictions=np.squeeze(np.asarray(predictions))\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='submission.csv')  \n",
    "pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "    \"submission.zip\", index_label=utils.ID_COLUMN, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
